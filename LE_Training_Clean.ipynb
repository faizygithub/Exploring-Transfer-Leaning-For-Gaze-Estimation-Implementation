{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "181c5804",
   "metadata": {},
   "source": [
    "# Left Eye (LE) - Luminance Estimation Model Training\n",
    "\n",
    "This notebook provides a clean, organized pipeline for training models for Left Eye luminance estimation.\n",
    "It follows a modular structure with clear sections for data loading, model architecture, and training.\n",
    "\n",
    "## Experiment Overview\n",
    "- **Dataset**: Left Eye images with luminance labels\n",
    "- **Task**: Regress two continuous values (luminance coordinates)\n",
    "- **Approaches**: Baseline model, Transfer Learning, From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f762d672",
   "metadata": {},
   "source": [
    "## Section 1: Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0297119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path for module imports\n",
    "# sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import custom modules\n",
    "from config import get_config\n",
    "from data_loader import load_images, CustomDataGenerator, get_augmentation_generator\n",
    "from model_architecture import create_and_compile_model\n",
    "from training_utils import train_model, plot_training_results, save_history_to_json, get_callbacks\n",
    "from system_monitoring import print_system_info\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f663c3f",
   "metadata": {},
   "source": [
    "## Section 2: System Information & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519019fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print system information\n",
    "print_system_info()\n",
    "\n",
    "# Get experiment configuration\n",
    "experiment = 'LE'  # Change to 'Face' or 'RE' for other experiments\n",
    "config = get_config(experiment)\n",
    "\n",
    "print(f\"\\nExperiment: {config['experiment_info']['name']}\")\n",
    "print(f\"Image Size: {config['image_size']}\")\n",
    "print(f\"Batch Size: {config['batch_size']}\")\n",
    "print(f\"Epochs: {config['epochs']}\")\n",
    "print(f\"Learning Rate: {config['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a1b31d",
   "metadata": {},
   "source": [
    "## Section 3: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb77866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get experiment info\n",
    "exp_info = config['experiment_info']\n",
    "\n",
    "print(\"Loading baseline training data...\")\n",
    "train_samples, train_labels = load_images(\n",
    "    exp_info['baseline'],\n",
    "    image_size=config['image_size']\n",
    ")\n",
    "\n",
    "print(\"\\nLoading test data...\")\n",
    "test_samples, test_labels = load_images(\n",
    "    exp_info['test'],\n",
    "    image_size=config['image_size']\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain samples shape: {train_samples.shape}\")\n",
    "print(f\"Train labels shape: {train_labels.shape}\")\n",
    "print(f\"Test samples shape: {test_samples.shape}\")\n",
    "print(f\"Test labels shape: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42af35cd",
   "metadata": {},
   "source": [
    "## Section 4: Create Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5620854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create augmentation generator\n",
    "datagen = get_augmentation_generator()\n",
    "\n",
    "# Create custom data generators\n",
    "train_generator = CustomDataGenerator(\n",
    "    train_samples, train_labels,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    datagen=datagen\n",
    ")\n",
    "\n",
    "test_generator = CustomDataGenerator(\n",
    "    test_samples, test_labels,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    datagen=None\n",
    ")\n",
    "\n",
    "print(f\"Training batches per epoch: {len(train_generator)}\")\n",
    "print(f\"Test batches per epoch: {len(test_generator)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83311a05",
   "metadata": {},
   "source": [
    "## Section 5: Train Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382e5807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = create_and_compile_model(\n",
    "    input_shape=(config['image_size'], config['image_size'], 3),\n",
    "    dense_units=96,\n",
    "    dropout_rate=config['dropout_rate'],\n",
    "    learning_rate=config['learning_rate']\n",
    ")\n",
    "\n",
    "print(\"Model created and compiled!\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98348891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get callbacks\n",
    "callbacks = get_callbacks(\n",
    "    patience_early_stopping=config['patience_early_stopping'],\n",
    "    patience_reduce_lr=config['patience_reduce_lr'],\n",
    "    factor_reduce_lr=config['factor_reduce_lr'],\n",
    "    min_learning_rate=config['min_learning_rate']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "results = train_model(\n",
    "    model,\n",
    "    train_generator,\n",
    "    test_generator,\n",
    "    epochs=config['epochs'],\n",
    "    callbacks=callbacks,\n",
    "    model_name=\"participant_baseline\"\n",
    ")\n",
    "\n",
    "history = results['history']\n",
    "training_time = results['time']\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time['hours']:.2f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c53a02c",
   "metadata": {},
   "source": [
    "## Section 6: Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e1b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training results\n",
    "plot_training_results(history, model_name=\"(participant_baseline)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f796a90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_mae = history.history['mae']\n",
    "val_mae = history.history['val_mae']\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Epochs trained: {len(train_loss)}\")\n",
    "print(f\"\\nFinal Training Loss: {train_loss[-1]:.6f}\")\n",
    "print(f\"Final Validation Loss: {val_loss[-1]:.6f}\")\n",
    "print(f\"\\nBest Validation Loss: {min(val_loss):.6f} (epoch {np.argmin(val_loss)+1})\")\n",
    "print(f\"Best Validation MAE: {min(val_mae):.6f} (epoch {np.argmin(val_mae)+1})\")\n",
    "print(f\"\\nTraining Time: {training_time['hours']:.2f} hours\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe6474c",
   "metadata": {},
   "source": [
    "## Section 7: Save Model & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab16abf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_name = 'participant_baseline'\n",
    "model.save(f'{model_name}.keras')\n",
    "print(f\"Model saved as {model_name}.keras\")\n",
    "\n",
    "# Save training history\n",
    "save_history_to_json(history, f'{model_name}.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05e42dc",
   "metadata": {},
   "source": [
    "## Section 8: Transfer Learning with Different Dataset Sizes\n",
    "\n",
    "Fine-tune the baseline model with different amounts of Ola-augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531441d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_name = 'participant_baseline'\n",
    "model.save(f'{model_name}.keras')\n",
    "print(f\"Model saved as {model_name}.keras\")\n",
    "\n",
    "# Save training history\n",
    "save_history_to_json(history, f'{model_name}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7592dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generators for transfer learning\n",
    "ola_train_gen = CustomDataGenerator(\n",
    "    ola_train_samples, ola_train_labels,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "ola_test_gen = CustomDataGenerator(\n",
    "    test_samples, test_labels,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Load baseline model for fine-tuning\n",
    "from tensorflow.keras.models import load_model\n",
    "model_ft = load_model(f'{model_name}.keras')\n",
    "\n",
    "# Fine-tune\n",
    "callbacks_ft = get_callbacks(\n",
    "    patience_early_stopping=config['patience_early_stopping'],\n",
    "    patience_reduce_lr=config['patience_reduce_lr'],\n",
    "    factor_reduce_lr=config['factor_reduce_lr'],\n",
    "    min_learning_rate=config['min_learning_rate']\n",
    ")\n",
    "\n",
    "results_ft = train_model(\n",
    "    model_ft,\n",
    "    ola_train_gen,\n",
    "    ola_test_gen,\n",
    "    epochs=config['epochs'],\n",
    "    callbacks=callbacks_ft,\n",
    "    model_name=f\"LE_Transfer_Ola_{dataset_key}\"\n",
    ")\n",
    "\n",
    "history_ft = results_ft['history']\n",
    "print(f\"\\nTransfer learning completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1315696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot transfer learning results\n",
    "plot_training_results(history_ft, model_name=f\"(LE Transfer - {dataset_key} Ola)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec91e52",
   "metadata": {},
   "source": [
    "## Section 9: From-Scratch Training with Ola Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d186cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new model for from-scratch training\n",
    "model_scratch = create_and_compile_model(\n",
    "    input_shape=(config['image_size'], config['image_size'], 3),\n",
    "    dense_units=96,\n",
    "    dropout_rate=config['dropout_rate'],\n",
    "    learning_rate=config['learning_rate']\n",
    ")\n",
    "\n",
    "callbacks_scratch = get_callbacks(\n",
    "    patience_early_stopping=config['patience_early_stopping'],\n",
    "    patience_reduce_lr=config['patience_reduce_lr'],\n",
    "    factor_reduce_lr=config['factor_reduce_lr'],\n",
    "    min_learning_rate=config['min_learning_rate']\n",
    ")\n",
    "\n",
    "results_scratch = train_model(\n",
    "    model_scratch,\n",
    "    ola_train_gen,\n",
    "    ola_test_gen,\n",
    "    epochs=config['epochs'],\n",
    "    callbacks=callbacks_scratch,\n",
    "    model_name=f\"participant_scratch_{dataset_key}\"\n",
    ")\n",
    "\n",
    "history_scratch = results_scratch['history']\n",
    "print(f\"\\nFrom-scratch training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44efc253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generators for transfer learning\n",
    "ola_train_gen = CustomDataGenerator(\n",
    "    ola_train_samples, ola_train_labels,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "ola_test_gen = CustomDataGenerator(\n",
    "    test_samples, test_labels,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Load baseline model for fine-tuning\n",
    "from tensorflow.keras.models import load_model\n",
    "model_ft = load_model(f'{model_name}.keras')\n",
    "\n",
    "# Fine-tune\n",
    "callbacks_ft = get_callbacks(\n",
    "    patience_early_stopping=config['patience_early_stopping'],\n",
    "    patience_reduce_lr=config['patience_reduce_lr'],\n",
    "    factor_reduce_lr=config['factor_reduce_lr'],\n",
    "    min_learning_rate=config['min_learning_rate']\n",
    ")\n",
    "\n",
    "results_ft = train_model(\n",
    "    model_ft,\n",
    "    ola_train_gen,\n",
    "    ola_test_gen,\n",
    "    epochs=config['epochs'],\n",
    "    callbacks=callbacks_ft,\n",
    "    model_name=f\"participant_transfer_{dataset_key}\"\n",
    ")\n",
    "\n",
    "history_ft = results_ft['history']\n",
    "print(f\"\\nTransfer learning completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9c16c3",
   "metadata": {},
   "source": [
    "## Section 10: Conclusions & Notes\n",
    "\n",
    "- **Baseline Model**: Trained on data without Ola augmentation\n",
    "- **Transfer Learning**: Fine-tunes baseline with Ola-augmented data\n",
    "- **From-Scratch**: Trains new model directly on Ola-augmented data\n",
    "\n",
    "Compare the results to determine which approach works best for your dataset."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
