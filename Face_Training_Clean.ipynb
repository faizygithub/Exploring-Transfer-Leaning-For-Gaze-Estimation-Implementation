{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae59ecf1",
   "metadata": {},
   "source": [
    "# Face - Feature Detection Model Training\n",
    "\n",
    "This notebook provides a clean, organized pipeline for training models for Face detection/feature estimation.\n",
    "It follows the same modular structure as the LE (Left Eye) experiment.\n",
    "\n",
    "## Experiment Overview\n",
    "- **Dataset**: Face images with feature labels\n",
    "- **Task**: Regress two continuous values (feature coordinates)\n",
    "- **Approaches**: Baseline model, Transfer Learning, From Scratch\n",
    "- **Note**: Same architecture and training strategy as LE, applied to Face data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551858a3",
   "metadata": {},
   "source": [
    "## Section 1: Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c75ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import custom modules\n",
    "from config import get_config\n",
    "from data_loader import load_images, CustomDataGenerator, get_augmentation_generator\n",
    "from model_architecture import create_and_compile_model\n",
    "from training_utils import train_model, plot_training_results, save_history_to_json, get_callbacks\n",
    "from system_monitoring import print_system_info\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400a0c3b",
   "metadata": {},
   "source": [
    "## Section 2: System Information & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d2da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print system information\n",
    "print_system_info()\n",
    "\n",
    "# Get experiment configuration for Face\n",
    "experiment = 'Face'\n",
    "config = get_config(experiment)\n",
    "\n",
    "print(f\"\\nExperiment: {config['experiment_info']['name']}\")\n",
    "print(f\"Image Size: {config['image_size']}\")\n",
    "print(f\"Batch Size: {config['batch_size']}\")\n",
    "print(f\"Epochs: {config['epochs']}\")\n",
    "print(f\"Learning Rate: {config['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c928680d",
   "metadata": {},
   "source": [
    "## Section 3: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b5a2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get experiment info\n",
    "exp_info = config['experiment_info']\n",
    "\n",
    "print(\"Loading baseline training data...\")\n",
    "train_samples, train_labels = load_images(\n",
    "    exp_info['baseline'],\n",
    "    image_size=config['image_size']\n",
    ")\n",
    "\n",
    "print(\"\\nLoading test data...\")\n",
    "test_samples, test_labels = load_images(\n",
    "    exp_info['test'],\n",
    "    image_size=config['image_size']\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain samples shape: {train_samples.shape}\")\n",
    "print(f\"Train labels shape: {train_labels.shape}\")\n",
    "print(f\"Test samples shape: {test_samples.shape}\")\n",
    "print(f\"Test labels shape: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1f44f2",
   "metadata": {},
   "source": [
    "## Section 4: Create Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a217059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create augmentation generator\n",
    "datagen = get_augmentation_generator()\n",
    "\n",
    "# Create custom data generators\n",
    "train_generator = CustomDataGenerator(\n",
    "    train_samples, train_labels,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    datagen=datagen\n",
    ")\n",
    "\n",
    "test_generator = CustomDataGenerator(\n",
    "    test_samples, test_labels,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    datagen=None\n",
    ")\n",
    "\n",
    "print(f\"Training batches per epoch: {len(train_generator)}\")\n",
    "print(f\"Test batches per epoch: {len(test_generator)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ad9f0b",
   "metadata": {},
   "source": [
    "## Section 5: Train Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813f3880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = create_and_compile_model(\n",
    "    input_shape=(config['image_size'], config['image_size'], 3),\n",
    "    dense_units=96,\n",
    "    dropout_rate=config['dropout_rate'],\n",
    "    learning_rate=config['learning_rate']\n",
    ")\n",
    "\n",
    "print(\"Model created and compiled!\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75309b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get callbacks\n",
    "callbacks = get_callbacks(\n",
    "    patience_early_stopping=config['patience_early_stopping'],\n",
    "    patience_reduce_lr=config['patience_reduce_lr'],\n",
    "    factor_reduce_lr=config['factor_reduce_lr'],\n",
    "    min_learning_rate=config['min_learning_rate']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "results = train_model(\n",
    "    model,\n",
    "    train_generator,\n",
    "    test_generator,\n",
    "    epochs=config['epochs'],\n",
    "    callbacks=callbacks,\n",
    "    model_name=\"participant_baseline\"\n",
    ")\n",
    "\n",
    "history = results['history']\n",
    "training_time = results['time']\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time['hours']:.2f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3216b34",
   "metadata": {},
   "source": [
    "## Section 6: Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eb32ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training results\n",
    "plot_training_results(history, model_name=\"(participant_baseline)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28fa45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Ola dataset (example with 500 samples)\n",
    "dataset_key = '500'  # Options: '100', '200', '300', '400', '500'\n",
    "\n",
    "print(f\"Loading Ola-augmented data ({dataset_key} samples)...\")\n",
    "ola_train_samples, ola_train_labels = load_images(\n",
    "    exp_info['training'][dataset_key],\n",
    "    image_size=config['image_size']\n",
    ")\n",
    "\n",
    "print(f\"Ola train samples shape: {ola_train_samples.shape}\")\n",
    "print(f\"Ola train labels shape: {ola_train_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c37b4b",
   "metadata": {},
   "source": [
    "## Section 7: Save Model & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c099e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_name = 'Face_Baseline'\n",
    "model.save(f'{model_name}.keras')\n",
    "print(f\"Model saved as {model_name}.keras\")\n",
    "\n",
    "# Save training history\n",
    "save_history_to_json(history, f'{model_name}.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0ad4a6",
   "metadata": {},
   "source": [
    "## Section 8: Transfer Learning with Different Dataset Sizes\n",
    "\n",
    "Fine-tune the baseline model with different amounts of Ola-augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff9ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_name = 'participant_baseline'\n",
    "model.save(f'{model_name}.keras')\n",
    "print(f\"Model saved as {model_name}.keras\")\n",
    "\n",
    "# Save training history\n",
    "save_history_to_json(history, f'{model_name}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d70d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generators for transfer learning\n",
    "ola_train_gen = CustomDataGenerator(\n",
    "    ola_train_samples, ola_train_labels,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "ola_test_gen = CustomDataGenerator(\n",
    "    test_samples, test_labels,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Load baseline model for fine-tuning\n",
    "from tensorflow.keras.models import load_model\n",
    "model_ft = load_model(f'{model_name}.keras')\n",
    "\n",
    "# Fine-tune\n",
    "callbacks_ft = get_callbacks(\n",
    "    patience_early_stopping=config['patience_early_stopping'],\n",
    "    patience_reduce_lr=config['patience_reduce_lr'],\n",
    "    factor_reduce_lr=config['factor_reduce_lr'],\n",
    "    min_learning_rate=config['min_learning_rate']\n",
    ")\n",
    "\n",
    "results_ft = train_model(\n",
    "    model_ft,\n",
    "    ola_train_gen,\n",
    "    ola_test_gen,\n",
    "    epochs=config['epochs'],\n",
    "    callbacks=callbacks_ft,\n",
    "    model_name=f\"participant_transfer_{dataset_key}\"\n",
    ")\n",
    "\n",
    "history_ft = results_ft['history']\n",
    "print(f\"\\nTransfer learning completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c6c167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot transfer learning results\n",
    "plot_training_results(history_ft, model_name=f\"(Face Transfer - {dataset_key} Ola)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21de837",
   "metadata": {},
   "source": [
    "## Section 9: From-Scratch Training with Ola Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b037651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new model for from-scratch training\n",
    "model_scratch = create_and_compile_model(\n",
    "    input_shape=(config['image_size'], config['image_size'], 3),\n",
    "    dense_units=96,\n",
    "    dropout_rate=config['dropout_rate'],\n",
    "    learning_rate=config['learning_rate']\n",
    ")\n",
    "\n",
    "callbacks_scratch = get_callbacks(\n",
    "    patience_early_stopping=config['patience_early_stopping'],\n",
    "    patience_reduce_lr=config['patience_reduce_lr'],\n",
    "    factor_reduce_lr=config['factor_reduce_lr'],\n",
    "    min_learning_rate=config['min_learning_rate']\n",
    ")\n",
    "\n",
    "results_scratch = train_model(\n",
    "    model_scratch,\n",
    "    ola_train_gen,\n",
    "    ola_test_gen,\n",
    "    epochs=config['epochs'],\n",
    "    callbacks=callbacks_scratch,\n",
    "    model_name=f\"participant_scratch_{dataset_key}\"\n",
    ")\n",
    "\n",
    "history_scratch = results_scratch['history']\n",
    "print(f\"\\nFrom-scratch training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c493c360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three approaches\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs_baseline = range(1, len(history.history['val_loss']) + 1)\n",
    "epochs_ft = range(1, len(history_ft.history['val_loss']) + 1)\n",
    "epochs_scratch = range(1, len(history_scratch.history['val_loss']) + 1)\n",
    "\n",
    "# Validation Loss\n",
    "axes[0].plot(epochs_baseline, history.history['val_loss'], 'b-o', label='Baseline', linewidth=2, markersize=3)\n",
    "axes[0].plot(epochs_ft, history_ft.history['val_loss'], 'g-s', label=f'Transfer (Ola-{dataset_key})', linewidth=2, markersize=3)\n",
    "axes[0].plot(epochs_scratch, history_scratch.history['val_loss'], 'r-^', label=f'From-Scratch (Ola-{dataset_key})', linewidth=2, markersize=3)\n",
    "axes[0].set_title('Validation Loss Comparison', fontsize=14)\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Validation MAE\n",
    "axes[1].plot(epochs_baseline, history.history['val_mae'], 'b-o', label='Baseline', linewidth=2, markersize=3)\n",
    "axes[1].plot(epochs_ft, history_ft.history['val_mae'], 'g-s', label=f'Transfer (Ola-{dataset_key})', linewidth=2, markersize=3)\n",
    "axes[1].plot(epochs_scratch, history_scratch.history['val_mae'], 'r-^', label=f'From-Scratch (Ola-{dataset_key})', linewidth=2, markersize=3)\n",
    "axes[1].set_title('Validation MAE Comparison', fontsize=14)\n",
    "axes[1].set_xlabel('Epochs')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7518420b",
   "metadata": {},
   "source": [
    "## Section 10: Conclusions & Notes\n",
    "\n",
    "- Same architecture and training strategy as the LE (Left Eye) experiment\n",
    "- **Baseline Model**: Trained on data without Ola augmentation\n",
    "- **Transfer Learning**: Fine-tunes baseline with Ola-augmented data\n",
    "- **From-Scratch**: Trains new model directly on Ola-augmented data\n",
    "\n",
    "Compare the results to determine which approach works best for Face region feature detection."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
